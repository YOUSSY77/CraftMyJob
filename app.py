# -*- coding: utf-8 -*-
"""
CraftMyJob â€“ Streamlit app for smart job suggestions
"""
import os
import io
import re
import time
import requests
import pandas as pd
import streamlit as st
from PIL import Image
from PyPDF2 import PdfReader
from docx import Document
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from requests.exceptions import RequestException
from rapidfuzz import fuzz
from datetime import datetime, timedelta

# â”€â”€ 0) CLEAN ENVIRONMENT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
for var in ("HTTP_PROXY", "http_proxy", "HTTPS_PROXY", "https_proxy"):
    os.environ.pop(var, None)

# â”€â”€ 1) STREAMLIT CONFIG & STYLING â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="CraftMyJob â€“ Job Seekers Hub France", layout="centered")
st.markdown("""
<style>
  .stButton>button { background-color:#2E86C1; color:white; border-radius:4px; }
  .section-header { font-size:1.2rem; font-weight:600; margin-top:1.5em; color:#2E86C1; }
  h1,h2,h3 { color:#2E86C1; }
  .offer-link a { color:#2E86C1; text-decoration:none; }
  .cv-summary { color:#1F8A70; }
</style>
""", unsafe_allow_html=True)
# Logo
try:
    logo = Image.open("logo_jobseekers.PNG")
    st.image(logo, width=120)
except:
    pass
st.title(" CraftMyJob â€“ Votre assistant emploi intelligent by Jobseeker HubFrance")

# â”€â”€ 2) DATA & MODEL PREP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data
def load_referentiel(path: str = "referentiel_metiers_craftmyjob_final.csv") -> pd.DataFrame:
    return pd.read_csv(path, dtype=str).fillna("")

@st.cache_data
def build_tfidf(df: pd.DataFrame, max_features: int = 2000):
    corpus = df["Activites"] + " " + df["Competences"] + " " + df["Metier"]
    vect = TfidfVectorizer(max_features=max_features)
    matrix = vect.fit_transform(corpus)
    return vect, matrix

referentiel = load_referentiel()
vecteur, tfidf_matrix = build_tfidf(referentiel)

# â”€â”€ 3) UTILITIES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def normalize_location(loc: str) -> str:
    if m := re.match(r"^(.+?) \((\d{5})\)", loc):
        return m.group(1)
    if m2 := re.match(r"DÃ©partement (\d{2})", loc):
        return m2.group(1)
    if m3 := re.match(r"^(.+) \(region:(\d+)\)", loc):
        return m3.group(1)
    return loc

def get_date_range(months: int = 2):
    end = datetime.now().date()
    start = end - timedelta(days=months * 30)
    return start.isoformat(), end.isoformat()

def search_territoires(query: str, limit: int = 10) -> list[str]:
    res = []
    if re.fullmatch(r"\d{2}", query):
        r = requests.get(
            f"https://geo.api.gouv.fr/departements/{query}/communes",
            params={"fields": "nom,codesPostaux", "limit": limit}, timeout=5
        )
        r.raise_for_status()
        for e in r.json():
            cp = e.get("codesPostaux", ["00000"])[0]
            res.append(f"{e['nom']} ({cp})")
        res.append(f"DÃ©partement {query}")
        return list(dict.fromkeys(res))
    r1 = requests.get(
        "https://geo.api.gouv.fr/communes",
        params={"nom": query, "fields": "nom,codesPostaux", "limit": limit}, timeout=5
    )
    if r1.status_code == 200:
        for e in r1.json():
            cp = e.get("codesPostaux", ["00000"])[0]
            res.append(f"{e['nom']} ({cp})")
    r2 = requests.get(
        "https://geo.api.gouv.fr/regions",
        params={"nom": query, "fields": "nom,code"}, timeout=5
    )
    if r2.status_code == 200:
        for rg in r2.json():
            res.append(f"{rg['nom']} (region:{rg['code']})")
    return list(dict.fromkeys(res))

def build_keywords(texts: list[str], max_terms: int = 10) -> str:
    combined = " ".join(texts).lower()
    tokens = re.findall(r"\w{2,}", combined)
    stop = {"et","ou","la","le","les","de","des","du","un","une",
            "Ã ","en","pour","par","avec","sans","sur","dans","au","aux"}
    seen, kws = set(), []
    for t in tokens:
        if t in stop or t in seen:
            continue
        seen.add(t); kws.append(t)
        if len(kws) >= max_terms:
            break
    return ",".join(kws)

def get_gpt_response(prompt: str, key: str) -> str:
    url = "https://api.openai.com/v1/chat/completions"
    headers = {"Authorization": f"Bearer {key}", "Content-Type": "application/json"}
    data = {
        "model": "gpt-3.5-turbo",
        "messages": [
            {"role": "system", "content": "Tu es un expert en recrutement et en personal branding."},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.7,
        "max_tokens": 800
    }
    r = requests.post(url, json=data, headers=headers, timeout=30)
    r.raise_for_status()
    return r.json()["choices"][0]["message"]["content"]

def fetch_ftoken(cid: str, secret: str) -> str:
    url = "https://entreprise.pole-emploi.fr/connexion/oauth2/access_token?realm=/partenaire"
    data = {
        "grant_type": "client_credentials",
        "client_id": cid,
        "client_secret": secret,
        "scope": "api_offresdemploiv2 o2dsoffre"
    }
    r = requests.post(url, data=data, timeout=10)
    r.raise_for_status()
    return r.json().get("access_token", "")

def search_offres(token: str, mots: str, lieu: str, limit: int = 5,
                  max_retries: int = 3, backoff_factor: float = 0.5) -> list:
    """
    Recherche les offres via l'API FranceTravail avec retry sur erreurs 5xx.
    """
    url = "https://api.francetravail.io/partenaire/offresdemploi/v2/offres/search"
    dateDebut, dateFin = get_date_range(2)
    params = {
        "motsCles": mots,
        "localisation": lieu,
        "range": f"0-{limit-1}",
        "dateDebut": dateDebut,
        "dateFin": dateFin,
        "tri": "dateCreation"
    }
    for attempt in range(1, max_retries + 1):
        try:
            r = requests.get(
                url,
                headers={"Authorization": f"Bearer {token}"},
                params=params,
                timeout=10
            )
            if 500 <= r.status_code < 600:
                raise RequestException(f"Server error {r.status_code}")
            if r.status_code == 204:
                return []
            if r.status_code not in (200, 206):
                st.error(f"FT API {r.status_code}: {r.text}")
                return []
            return r.json().get("resultats", [])
        except RequestException:
            if attempt == max_retries:
                st.warning(
                    "âš ï¸ ProblÃ¨me technique avec lâ€™API FranceTravail â€” "
                    "certaines offres n'ont pas pu Ãªtre rÃ©cupÃ©rÃ©es."
                )
                return []
            time.sleep(backoff_factor * (2 ** (attempt - 1)))

def filter_by_location(offers: list, loc_norm: str) -> list:
    out = []
    cp_norm = loc_norm.lower()
    for o in offers:
        lib = o.get('lieuTravail', {}).get('libelle', "").lower()
        cp  = str(o.get('lieuTravail', {}).get('codePostal', ""))
        if cp_norm in lib or cp_norm == cp:
            out.append(o)
    return out

def scorer_metier(inp: dict, df: pd.DataFrame, top_k: int = 6) -> pd.DataFrame:
    doc = f"{inp['missions']} {inp['skills']} {inp['job_title']}"
    v_user = vecteur.transform([doc])
    cos = cosine_similarity(v_user, tfidf_matrix).flatten()
    df2 = df.copy()
    df2['cosine'] = cos
    df2['fz_t'] = df2['Metier'].apply(lambda m: fuzz.token_set_ratio(m, inp['job_title']) / 100)
    df2['fz_m'] = df2['Activites'].apply(lambda a: fuzz.token_set_ratio(a, inp['missions']) / 100)
    df2['fz_c'] = df2['Competences'].apply(lambda c: fuzz.token_set_ratio(c, inp['skills']) / 100)
    df2['score'] = (0.5*df2['cosine'] + 0.2*df2['fz_t'] +
                    0.15*df2['fz_m'] + 0.15*df2['fz_c']) * 100
    return df2.nlargest(top_k, 'score')

# â”€â”€ 3.a) UTILITIES POUR TOP 4 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def generate_title_variants(title: str) -> list[str]:
    variants = {title.strip()}
    if not title.endswith("s"):
        variants.add(f"{title}s")
    else:
        variants.add(title.rstrip("s"))
    synmap = {
        "manager": ["responsable", "chef"],
        "assistant": ["adjoint"],
        "developer": ["dÃ©veloppeur"],
        "engineer": ["ingÃ©nieur"],
    }
    for mot in title.lower().split():
        if mot in synmap:
            variants.update(synmap[mot])
    return list(variants)

def select_discriminant_skills(skills: str,
                               vectorizer: TfidfVectorizer,
                               top_n: int = 5) -> list[str]:
    tokens = re.findall(r"\w{2,}", skills.lower())
    features = vectorizer.get_feature_names_out()
    idf_vals = vectorizer.idf_
    idf_map = dict(zip(features, idf_vals))
    scored = [(t, idf_map.get(t, 0)) for t in set(tokens)]
    scored = sorted(scored, key=lambda x: x[1], reverse=True)
    return [t for t,_ in scored[:top_n]]

# â”€â”€ 4) PROFILE FORM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.header("1ï¸âƒ£ Profil & prÃ©fÃ©rences")
cv_text = ""
up = st.file_uploader("ğŸ“‚ CV (optionnel)", type=["pdf","docx","txt"])
if up:
    ext = up.name.rsplit('.', 1)[-1].lower()
    if ext == 'pdf':
        cv_text = ' '.join(p.extract_text() or '' for p in PdfReader(up).pages)
    elif ext == 'docx':
        cv_text = ' '.join(p.text for p in Document(up).paragraphs)
    else:
        cv_text = up.read().decode(errors='ignore')

job_title = st.text_input("ğŸ”¤ Poste souhaitÃ©")
missions  = st.text_area("ğŸ“‹ Missions principales")
skills    = st.text_area("ğŸ§  CompÃ©tences clÃ©s")

st.markdown("<div class='section-header'>ğŸŒ Territoires</div>", unsafe_allow_html=True)
typed = st.text_input("Tapez commune/dÃ©partement/rÃ©gionâ€¦")
opts  = search_territoires(typed) if typed else []
default_locs = st.session_state.get('locations', [])
sel   = st.multiselect("SÃ©lectionnez vos territoires", options=(default_locs+opts), default=default_locs)
st.session_state['locations'] = sel

exp_level = st.radio("ğŸ¯ ExpÃ©rience", ["DÃ©butant (0-2 ans)", "ExpÃ©rimentÃ© (2-5 ans)", "Senior (5+ ans)"])
contract = st.multiselect("ğŸ“„ Types de contrat", ["CDI","CDD","Freelance","Stage","Alternance"],
                          default=["CDI","CDD","Freelance"])
remote   = st.checkbox("ğŸ  Full remote")

# â”€â”€ 5) CLÃ‰S API & IA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.header("2ï¸âƒ£ ClÃ©s API & IA")
key_openai    = st.text_input("ğŸ”‘ OpenAI API Key", type="password")
key_pe_id     = st.text_input("ğŸ”‘ PÃ´le-Emploi Client ID", type="password")
key_pe_secret = st.text_input("ğŸ”‘ PÃ´le-Emploi Client Secret", type="password")

tpls = {
    'ğŸ“„ Bio LinkedIn':    'RÃ©dige une bio LinkedIn professionnelle.',
    'âœ‰ï¸ Mail de candidature': 'Ã‰cris un mail de candidature spontanÃ©e.',
    'ğŸ“ƒ Mini CV':         'GÃ©nÃ¨re un mini-CV (5-7 lignes).'
}
choices = st.multiselect("GÃ©nÃ©rations IA", list(tpls.keys()), default=list(tpls.keys())[:2])

# â”€â”€ 6) ACTION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if st.button("ğŸš€ Lancer tout"):
    if not key_openai:
        st.error("ğŸ”‘ ClÃ© OpenAI requise")
        st.stop()
    if not (key_pe_id and key_pe_secret and sel):
        st.error("ğŸ”‘ Identifiants PÃ´le-Emploi et territoires requis")
        st.stop()

    profile = { 'job_title': job_title, 'missions': missions, 'skills': skills }

    # â€” 6.1) RÃ©sumÃ© CV
    if cv_text:
        prompt_summary = f"RÃ©sumÃ© en 5 points clÃ©s du CV suivant:\n{cv_text[:1000]}"
        summary = get_gpt_response(prompt_summary, key_openai)
        st.markdown("**RÃ©sumÃ© CV:**", unsafe_allow_html=True)
        for line in summary.split('\n'):
            st.markdown(f"- <span class='cv-summary'>{line.strip()}</span>",
                        unsafe_allow_html=True)

    # â€” 6.2) GÃ©nÃ©rations IA
    st.header("ğŸ§  GÃ©nÃ©ration IA")
    for name in choices:
        instruction = tpls[name]
        if name == 'ğŸ“„ Bio LinkedIn':
            instruction += ' Ne mentionne aucune localisation ni anciennetÃ©, limite Ã  4 lignes.'
        prompt_lines = [
            f"Poste: {job_title}",
            f"Missions: {missions}",
            f"CompÃ©tences: {skills}",
        ]
        if cv_text:
            prompt_lines.append(f"RÃ©sumÃ© CV: {summary[:300]}")
        prompt_lines += [
            f"Territoires: {', '.join(sel)}",
            f"ExpÃ©rience: {exp_level}",
            f"Contrat(s): {', '.join(contract)}",
            f"TÃ©lÃ©travail: {'Oui' if remote else 'Non'}",
            '', instruction
        ]
        prompt = "\n".join(prompt_lines)
        try:
            res = get_gpt_response(prompt, key_openai)
            st.subheader(name)
            st.markdown(res)
        except requests.HTTPError as e:
            if e.response.status_code == 401:
                st.error("ClÃ© OpenAI invalide ou expirÃ©e.")
            else:
                st.error(f"Erreur OpenAI ({e.response.status_code}) : {e.response.text}")
            st.stop()

    # â€” 6.3) Token PÃ´le-Emploi
    try:
        token = fetch_ftoken(key_pe_id, key_pe_secret)
    except requests.HTTPError as e:
        status = e.response.status_code
        if status == 401:
            st.error("ğŸ”‘ Identifiants PÃ´le-Emploi invalides ou expirÃ©s.")
        else:
            st.error(f"Erreur PÃ´le-Emploi (code {status}) : {e.response.text}")
        st.stop()

    # â€” 6.4) Mots-clÃ©s ATS
    st.header("ğŸ”‘ Mots-clÃ©s recommandÃ©s pour lâ€™ATS")
    best_metier = scorer_metier(profile, referentiel, top_k=1).iloc[0]
    kws_ats = build_keywords([best_metier['Activites'], best_metier['Competences']], max_terms=10)
    for kw in kws_ats.split(','):
        st.markdown(f"- {kw}")

    # â€” 6.5) Top Offres (requÃªte ciblÃ©e)
    st.header(f"4ï¸âƒ£ Top offres pour Â« {job_title} Â»")
    variants    = generate_title_variants(job_title)
    disc_skills = select_discriminant_skills(skills, vecteur, top_n=5)
    keywords    = " ".join(variants + disc_skills)
    all_offres  = []
    for loc in sel:
        loc_norm = normalize_location(loc)
        offs = search_offres(token, keywords, loc_norm, limit=10)
        offs = filter_by_location(offs, loc_norm)
        all_offres.extend(offs)
    all_offres = [o for o in all_offres if o.get('typeContrat','') in contract]
    for o in all_offres:
        o['sim_title'] = fuzz.token_set_ratio(o.get('intitule',''), job_title)
    top4 = sorted(all_offres, key=lambda x: x['sim_title'], reverse=True)[:4]
    if top4:
        for o in top4:
            title = o.get('intitule','â€“')
            lib   = o['lieuTravail']['libelle']
            cp    = o['lieuTravail']['codePostal']
            typ   = o.get('typeContrat','â€“')
            url   = o.get('contact',{}).get('urlPostulation') or o.get('contact',{}).get('urlOrigine','')
            st.markdown(
                f"**{title}** ({typ}) â€“ {lib} [{cp}]  \n"
                f"<span class='offer-link'><a href='{url}' target='_blank'>Voir l'offre</a></span>\n---",
                unsafe_allow_html=True
            )
    else:
        st.info("Aucune offre pertinente trouvÃ©e pour ce poste.")

    # â€” 6.6) SIS â€“ MÃ©tiers recommandÃ©s
    st.header("5ï¸âƒ£ SIS â€“ MÃ©tiers recommandÃ©s")
    top6 = scorer_metier(profile, referentiel, top_k=6)
    for _, r in top6.iterrows():
        st.markdown(f"**{r['Metier']}** â€“ {int(r['score'])}%")
        subs = []
        for loc in sel:
            loc_norm = normalize_location(loc)
            tmp = search_offres(token, r['Metier'], loc_norm, limit=3)
            tmp = filter_by_location(tmp, loc_norm)
            subs.extend(tmp)
        subs = [o for o in subs if o.get('typeContrat','') in contract]
        seen2 = set()
        if subs:
            for o in subs:
                url2 = o.get('contact',{}).get('urlPostulation') or o.get('contact',{}).get('urlOrigine','')
                if url2 not in seen2:
                    seen2.add(url2)
                    dt   = o.get('dateCreation','')[:10]
                    lib  = o['lieuTravail']['libelle']
                    typ  = o.get('typeContrat','â€“')
                    desc = (o.get('description','') or '').replace('\n',' ')[:150] + 'â€¦'
                    st.markdown(
                        f"â€¢ **{o['intitule']}** ({typ}) â€“ {lib} (_PubliÃ© {dt}_)  \n"
                        f"{desc}  \n"
                        f"<span class='offer-link'><a href='{url2}' target='_blank'>Voir / Postuler</a></span>",
                        unsafe_allow_html=True
                    )
        else:
            st.info("Aucune offre trouvÃ©e pour ce mÃ©tier dans vos territoires et contrats.")
